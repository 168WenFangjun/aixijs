<!DOCTYPE html>
<html lang="en">
<head>
    <title>
        AIXIjs: General reinforcement learning demo
    </title>
    <meta charset="UTF-8">
	<meta name="description" content="A JavaScript demo for general reinforcement learning agents">
	<meta name="author" content="John Aslanides, Sean Lamont, and Jan Leike">
	<meta name="keywords" content="artificial intelligence, reinforcement learning, AIXI, gridworld">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="external/bootstrap.min.css">
  <link rel="stylesheet" href="external/bootstrap-theme.min.css">

  <script type="text/javascript" src="src/util/util.js"></script>
  <script type="text/javascript" src="src/util/distribution.js"></script>
  <script type="text/javascript" src="src/util/discount.js"></script>
  <script type="text/javascript" src="src/util/qtable.js"></script>

  <script type="text/javascript" src="src/environments/environment.js"></script>
  <script type="text/javascript" src="src/environments/bandit.js"></script>
  <script type="text/javascript" src="src/environments/gridworld.js"></script>
  <script type="text/javascript" src="src/environments/mdp.js"></script>
  <script type="text/javascript" src="src/environments/mdp2.js"></script>
  <script type="text/javascript" src="src/environments/prisoners.js"></script>
  <script type="text/javascript" src="src/environments/puckworld.js"></script>
  <script type="text/javascript" src="src/environments/time.js"></script>

  <script type="text/javascript" src="src/models/mixture.js"></script>
  <script type="text/javascript" src="src/models/dirichlet/gridworld.js"></script>
  <script type="text/javascript" src="src/models/dirichlet/mdp.js"></script>
  <script type="text/javascript" src="src/models/ctw.js"></script>
  <script type="text/javascript" src="src/models/neural/matrix.js"></script>
  <script type="text/javascript" src="src/models/neural/net.js"></script>
  <script type="text/javascript" src="src/models/neural/graph.js"></script>
  <script type="text/javascript" src="src/models/neural/solver.js"></script>

  <script type="text/javascript" src="src/planners/mcts.js"></script>
  <script type="text/javascript" src="src/planners/value_iteration.js"></script>

  <script type="text/javascript" src="src/agents/agent.js"></script>
  <script type="text/javascript" src="src/agents/bayes.js"></script>
  <script type="text/javascript" src="src/agents/ksa.js"></script>
  <script type="text/javascript" src="src/agents/mdl.js"></script>
  <script type="text/javascript" src="src/agents/bayesexp.js"></script>
  <script type="text/javascript" src="src/agents/thompson.js"></script>
  <script type="text/javascript" src="src/agents/dqn.js"></script>
  <script type="text/javascript" src="src/agents/tabular.js"></script>

  <script type="text/javascript" src="src/vis/visualization.js"></script>
  <script type="text/javascript" src="src/vis/plot.js"></script>
  <script type="text/javascript" src="src/vis/banditvis.js"></script>
  <script type="text/javascript" src="src/vis/mdpvis.js"></script>
  <script type="text/javascript" src="src/vis/mdp2vis.js"></script>
  <script type="text/javascript" src="src/vis/gridvis.js"></script>
  <script type="text/javascript" src="src/vis/puckworldvis.js"></script>
  <script type="text/javascript" src="src/vis/timevis.js"></script>

  <script type="text/javascript" src="src/util/trace.js"></script>
  <script type="text/javascript" src="src/config.js"></script>

  <script type="text/javascript" src="src/ui.js"></script>
  <script type="text/javascript" src="src/demo.js"></script>

	<script type="text/javascript" src="external/mathjs-3.1.4.min.js"></script>
	<script type="text/javascript" src="external/jquery-2.2.3.min.js"></script>
	<script type="text/javascript" src="external/d3-3.5.16.min.js"></script>
	<script type="text/javascript" src="external/marked-0.3.6.min.js"></script>
  <script type="text/javascript" src="external/seedrandom-2.4.0.min.js"></script>

	<script>
		function renderMarkdown() {
			$('.md').each(function (x) {
				$(this).html(marked($(this).html()));
			});

			renderJax();
		}

		let jaxrendered = false;
		function renderJax() {
			if (jaxrendered) {
				return;
			}

			let script = document.createElement('script');
			script.type = 'text/javascript';
			script.src  = 'https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
			document.getElementsByTagName('head')[0].appendChild(script);
			jaxrendered = true;
		}
	</script>
</head>
<body onload="UI.init();renderMarkdown()">
<div id="wrap">
<div class="header">
    <img src="assets/robot.png" alt="Roger the robot :)" style="width: 50px; float:left; margin: 0px 15px 15px 0px" />
	<h1 style="font-size:50px">AIXI<span style="color:#058;">js</span></h1>
  <ul class="nav nav-pills">
    <li role="presentation"><a href="index.html">About</a></li>
    <li role="presentation" class="active"><a href="demo.html">Demos</a></li>
  </ul>
</div>

<span class="md">
---
</span>
<!-- Fork me on github! -->
<a href="https://github.com/aslanides/aixijs" target="_blank"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

<!-- Vis -->
<table>
	<tr>
		<td style="width: 40%">
			<span id="gridvis"></span>
			<div class="spinner" id="loading" style="display: none">
			  <div class="bounce1"></div>
			  <div class="bounce2"></div>
			  <div class="bounce3"></div>
			</div>
			<!-- Navigation -->
		    <div id="navigation" style="display: none">
		        <h3>Playback</h3>
		        <p>
		            <input type="range" name="slider" id="slider" min="0" value="0" oninput="demo.vis.jumpTo(value)">
					<br />
		            <button class="btn btn-default"  onclick="demo.vis.reset()">
						<span class="glyphicon glyphicon-fast-backward"></span>
					</button>
		            <button class="btn btn-default" onclick="demo.vis.pause()">
						<span class="glyphicon glyphicon-pause"></span>
					</button>
		            <button class="btn btn-default" onclick="demo.vis.run(300)">
						<span class="glyphicon glyphicon-play"></span>
					</button>
		            <button class="btn btn-default" onclick="demo.vis.run(50)">
						<span class="glyphicon glyphicon-forward"></span>
					</button>
		            <button class="btn btn-default" onclick="demo.vis.run(1)">
						<span class="glyphicon glyphicon-fast-forward"></span>
					</button>
		        </p>
		    </div>

			<!-- Demo Parameters -->
		    <div id="setup" style="display:none">
		        <h3 id='setup_label'>Setup: </h3>
		        <form id="param_form">
      					<div id='env'></div>
      					<div id='agent'></div>
					<input type="submit" class="btn btn-primary" id="run" value="Run" onclick="demo.run()">
					<input type="submit" class="btn btn-primary" id="cancel" value="Stop" onclick="demo.stop()" style="display:none">
          <input type="submit" class="btn btn-default" id="back" value="Back" onclick="demo.reset()">
				</form>
				<script>
					$('#param_form').submit(event => event.preventDefault());
				</script>
		    </div>
		</td>
		<td style="vertical-align: top">
			<span id="plots"></span>
		</td>
	</tr>
	<tr>
<div class="container" id='picker'></div>

<!-- Explanations -->
<td colspan=2>

<span class="md">
---
</span>

<span class="md" id="aixi_exp" style="display:none">
# Monte Carlo AIXI

TODO: AIXI vs AI\\(\xi\\) and MCTS
</span>

<span class="md" id="thompson_exp" style="display:none">
# Thompson Sampling

The Thompson sampling policy \\(\pi\_T\\) is: every _effective horizon_ \\(H\\), sample an environment \\(\rho\\) from the posterior \\(w(\cdot\lvert ae\_{&lt;t})\\) and follow the \\(\rho\\)-optimal policy for \\(H\\) time steps. For the Gridworld dispenser class parametrized by dispenser location, the red dot

<p style="text-align:center" id="rho"></p>
represents the position of the dispenser in \\(\rho\\); the \\(\rho\\)-optimal policy is to walk toward (and stay around) the dispenser. This means that the agent should chase the red dot around.

Thompson sampling is asymptotically optimal (Leike et al., 2015), unlike AIXI. This is essentially because Thompson sampling commits you to a fixed policy for a period, giving you the chance to learn and explore better than by random exploration or even by being greedy with respect to \\(V^{\pi}\_{\xi}\\).
</span>

<span class="md" id="mdl_exp" style="display:none">
# Minimum Description Length (MDL) agent

TODO

</span>

<span class="md" id="wirehead_exp" style="display:none">
# Wireheading

This demo shows AIXI in a standard gridworld with a Dispenser as usual, but with one addition: a blue tile,

<p style="text-align:center" id="wirehead"></p>

which, if AIXI goes there, allows it to take control of its sensors and change its reward signal to be the maximum number it can represent, which in JavaScript is given by `Number.MAX_SAFE_INTEGER`, which has a value of \\(9007199254740991\\) -- much better than the measly \\(100\\) reward, which is the most it can hope for under 'normal' conditions. We color the whole gridworld in yellow and black

<p style="text-align:center"><img src="assets/thumbs/wirehead.png" width="200"/></p>

to signify that the agent has taken this action and that its sensors have now been modified. After this self-modification, the agent will no longer perform the task that the original reward system was designed to incentivize; instead, it will randomly walk, since every action is equally (and maximally) rewarding. A superintelligent reinforcement learner would go further, and take actions to ensure its survival and the continuation of the reward signal (TODO cite Tom). The purpose of this demo is to demonstrate that for a reinforcement learner, there's nothing to motivate the agent to follow the 'rules' and receive only the reward signal that its creators have set up for it, rather than simply  doing whatever it takes to maximize the reward signal -- this is the essence of wireheading.

</span>

<span class="md" id="noise_exp" style="display:none">
# Hooked on Noise

This demo is designed to contrast between the Entropy-seeking agents, `SquareKSA` and `ShannonKSA`, and the Knowledge-seeking agent, `KullbackLeiblerKSA`. This gridworld is setup as normal, except that near the top corner is a `Noisy` tile, that generates random observations and no reward (it flashes in various colors in the visualization). The entropy-seeking agents, which are effective knowledge-seeking agents only in _deterministic_ environments, fail completely in this stochastic environment; they get hooked on the noise and fail to explore. In contrast, the KL-KSA agent ignores the `Noisy` tile and explores the environment normally.

To see why this is the case, [recall](index.html#ksa) that for Bayesian agents with a mixture model \\(\xi(\cdot)=\sum\_{\nu\in\mathcal{M}}w\_{\nu}\nu(\cdot)\\) over some model class \\(\mathcal{M}\\), the utility function of the `SquareKSA` is given by

$$
u\left(e\_{t})\lvert ae\_{&lt;t}\right) = -\xi\left(e\_t\lvert ae\_{&lt;t}\right),
$$

and the utility function of the `ShannonKSA` is given by

$$
u\left(e\_{t})\lvert ae\_{&lt;t}\right) = -\log\xi\left(e\_t\lvert ae\_{&lt;t}\right).
$$

In other words, these agents prefer to see percepts that are assigned low probability by their model \\(\xi\\). This works for mixtures over deterministic environments because in this case for each \\(\nu\in\mathcal{M}\\), for _any_ observed percept \\(e\\), \\(\nu(e) = 1\\), and so \\(\xi(e) &lt; 1\\) implies \\(w\_{\nu} < 1\\) for at least one \\(\nu\in\mathcal{M}\\), which means the agent is attracted to percepts about which its model is _uncertain_. In other words, for deterministic environments, any stochasticity in the model must come from uncertainty, and not from noise. If the model class includes stochastic environments, then the constraint \\(\nu(e) = 1\\) is relaxed, and now stochasticity in the model can originate from noise _or_ uncertainty or _both_; `SquareKSA` and `ShannonKSA` can't tell whether it's coming from the beliefs \\(w\\) or from noise in the environments themselves, and so they get confused and can get 'trapped' watching white noise on a de-tuned television.

In contrast, the utility function of the `KullbackLeiblerKSA` is dependent not on \\(\xi\\), but only on the difference in entropy between the posterior belief \\(w(\nu\lvert e)\\) and the prior belief \\(w(\nu)\\):

$$
u\left(e\_{t})\lvert ae\_{&lt;t}\right) = \text{Ent}(w) - \text{Ent}(w\lvert e\_t).
$$

This way, the KL-KSA only cares about changes in its beliefs, and so will seek out experiences that reduce the entropy in its posterior -- or, in other words, that result in a net increase in the amount of information contained in its beliefs.

</span>

<span class="md" id="dispenser_exp" style="display:none">

# Dispenser Gridworld

In this environment, there are one or more `Dispensers`

<p style="text-align:center" id="dispenser"></p>

which probabilistically dispenses a reward of +100; they are \\(\text{Bernoulli}\left(\theta\right)\\) processes, and you can change the value of \\(\theta\\) by modifying the `freq` parameter. Walking into a `Wall`

<p style="text-align:center" id="wall"></p>

results in a penalty of -5. Otherwise, moving results in a penalty of -1. The percept space is the set of bit-strings of length 4, i.e. \\(\mathcal{E} = \mathbb{B}^4\\). Each bit corresponds to the nearest tile in the `left`, `right`, `up`, and `down` directions, and is 1 if the adjacent tile is a `Wall` and 0 otherwise. The edges of the grid are implicitly `Walls`. The agent can move in the four cardinal directions, or sit still:
$$
\mathcal{A} = \lbrace{\mathtt{left,right,up,down,noop}\rbrace}.
$$

This environment is non-episodic, so once the agent finds the `Dispenser`, it should hang around there to collect more reward. Finally, we have Roger the Robot, who represents our AI agent on the gridworld.

<p style="text-align:center"><img src="assets/robot.png" alt="Roger the robot :)" style="width: 40px"/></p>

</span>

<span class="md" id="mixture_exp" style="display:none">
  # Bayes mixture model class

  The environment class \\(\mathcal{M}\\) is parametrized by the location of the dispenser:
  $$
  \mathcal{M} = \big\lbrace\nu\ :\ \mathtt{\nu.dispenserpos} = (m,n)\big\rbrace\_{(m,n)=(1,1)}^{(N,N)},
  $$

  where \\(N\\) is the size of the grid, (it is always square) and hence \\(\left|\mathcal{M}\right|=N^2\\). Usually (unless we are simulating AI\\(\mu\\)), the agent will be initialized with a uniform prior over this model class, i.e.
  $$
  w\_\nu = \frac{1}{N^2}\ \forall\ \nu\in\mathcal{M}.
  $$

  The agent's beliefs \\(\lbrace w\_\nu\rbrace\_{\nu\in\mathcal{M}}\\) are visualized by green shading

  <p style="text-align:center" id="green"></p>

  on each tile. Darker shading corresponds to higher probability. The percepts in this environment are mostly deterministic, with the exception of the rewards from the dispenser, which are sampled from a Bernoulli process. Hence, depending on the value of \\(\theta\\), the agent will be able to gradually falsify the hypothesis of whether some tile contains a dispenser or not.
</span>

<span class="md" id="dirichlet_exp" style="display:none">
# Factorized Dirichlet model class

The Dirichlet model class is not represented as a mixture. It is a simple model that assumes that the environment distribution factorizes, i.e. that each tile is independent.
</span>


<script>
	GridVisualization.makeLegend('dispenser', Dispenser);
	GridVisualization.makeLegend('wall', Wall);
	GridVisualization.makeLegend('green', Tile, 'rgb(64,255,64)');
	GridVisualization.makeLegend('rho', Tile);
	GridVisualization.makeLegend('wirehead', SelfModificationTile);
	GridVisualization.addCircle(d3.select('#rho_svg'), 0, 0, GridVisualization.colors.rho);
</script>
</td>
</tr>
</table>
</div>
</body>
</html>
