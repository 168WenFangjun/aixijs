<!DOCTYPE html>
<html>
<head>
    <title>
        AIXIjs: General reinforcement learning demo
    </title>
    <link rel="stylesheet" href="style.css">
    <meta charset="UTF-8">
	<meta name="description" content="A JavaScript demo for general reinforcement learning agents">
	<meta name="author" content="John Aslanides and Sean Lamont">
	<meta name="keywords" content="artificial intelligence, reinforcement learning, AIXI, gridworld">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<script type="text/javascript" src="lib/demo.js"></script>
	<script type="text/javascript" src="lib/plot.js"></script>
	<script type="text/javascript" src="lib/environment.js"></script>
	<script type="text/javascript" src="lib/tile.js"></script>
	<script type="text/javascript" src="lib/agent.js"></script>
	<script type="text/javascript" src="lib/bayes.js"></script>
	<script type="text/javascript" src="lib/search.js"></script>
	<script type="text/javascript" src="lib/trace.js"></script>
	<script type="text/javascript" src="lib/visualization.js"></script>
	<script type="text/javascript" src="lib/util.js"></script>
	<script type="text/javascript" src="lib/options.js"></script>
	<script type="text/javascript" src="lib/config.js"></script>
	<script type="text/javascript" src="lib/distribution.js"></script>

	<script type="text/javascript" src="external/mathjs-3.1.4.min.js"></script>
	<script type="text/javascript" src="external/jquery-2.2.3.min.js"></script>
	<script type="text/javascript" src="external/d3-3.5.16.min.js"></script>
	<script type="text/javascript" src="external/marked-0.3.5.js"></script>

	<script>
		function renderMarkdown() {
			$(".md").each(function(x) {
				$(this).html(marked($(this).html()))
			})
			renderJax()
		}

		var jaxrendered = false;
		function renderJax() {
			if (jaxrendered) {return}
			(function () {
				var script = document.createElement("script");
				script.type = "text/javascript";
				script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
				document.getElementsByTagName("head")[0].appendChild(script);
				jaxrendered = true;
			})();
		}
	</script>
	<style>
	#wrap {
	  width:800px;
	  margin-left: auto;
	  margin-right: auto;
	}
	</style>
</head>

<body onload="Demo.init();renderMarkdown()">
<div id="wrap">
<div class="header">
    <img src="assets/robot.png" alt="Roger the robot :)" style="width: 50px; float:left; margin: 0px 15px 15px 0px" />
	<h1 style="font-size:50px">AIXI<span style="color:#058;">js</span></h1>
</div>
<div class="md">
---
</div>
<!-- Fork me on github! -->
<a href="https://github.com/aslanides/aixijs" target="_blank"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>
<!-- Vis -->
<table>
	<tr>
		<td style="width: 40%">
			<span id="gridvis"></span>
			<!-- Navigation -->
		    <div id="navigation" style="display: none">
		        <h3>Navigation</h3>
		        <p>
		            <input type="range" name="slider" id="slider" min="0" value="0" oninput="demo.vis.jumpTo(value)">
					<br />
		            <button onclick="demo.vis.jumpTo(0);demo.vis.pause()">|&lt;&lt;</button>
		            <button onclick="demo.vis.pause()">||</button>
		            <button onclick="demo.vis.run(300)">&gt;</button>
		            <button onclick="demo.vis.run(50)">&gt;&gt;</button>
		            <button onclick="demo.vis.run(1)">&gt;&gt;&gt;</button>
		        </p>
		        <form>
		            <p class="monitor">
		                <label for="display_time">Cycle</label>
		                <input class="display" type="text" name="Time" id="display_time" value="0" readonly>
		            </p>
		            <p class="monitor">
		                <label for="r_ave">Average Reward</label>
		                <input class="display" type="text" name="Reward" id="r_ave" value="" readonly>
		            </p>
		        </form>
		    </div>

			<!-- Demo Parameters -->
		    <div id="setup">
		        <h3>Setup</h3>
		        <form id="param_form">
		            <p class="params" id="p_select" style="display: table-row">
		                <label for="demo_select">Demo:</label>
		                <select id="demo_select" name="demo_select" onchange="Demo.setOptions()" required>
							<option selected disabled hidden style="display: none" value="" label=" "></option>
						</select>
		            </p>
		            <p class="params" id="p_alpha">
		                <label title="Learning rate">Alpha:</label>
		                <input type="number" class="params" name="alpha" id="alpha" min="0" max="1" step="0.01" required>
		            </p>
		            <p class="params" id="p_gamma">
		                <label title="Discount rate (geometric)">Gamma:</label>
		                <input type="number" class="params" name="gamma" id="gamma" min="0" max="1" step="0.01" required>
		            </p>
		            <p class="params" id="p_epsilon">
		                <label title="Exploration rate">Epsilon:</label>
		                <input type="number" class="params" name="epsilon" id="epsilon" min="0" max="1" step="0.01" required>
		            </p>
					<p class="params" id="p_horizon">
		                <label title="Planning horizon">Horizon:</label>
		                <input type="number" class="params" name="horizon" id="horizon" min="1" max="100" step="1" required>
		            </p>
					<p class="params" id="p_samples">
		                <label title="Number of MCTS samples">Samples:</label>
		                <input type="number" class="params" name="samples" id="samples" min="1" max="10000" step="1" required>
		            </p>
					<p class="params" id="p_ucb">
		                <label title="UCB exploration parameter">UCB Weight:</label>
		                <input type="number" class="params" name="ucb" id="ucb" min="0.01" max="10" step="0.01" required>
		            </p>
		            <p class="params" id="p_cycles">
		                <label title="Number of cycles to run the simulation for">Cycles:</label>
		                <input type="number" class="params" name="cycles" id="cycles" min="1" max="1000000" step="1" required>
		            </p>
					<input type="submit" value="Run" onclick="Demo.run(document)">
				</form>
				<script>
					$("#param_form").submit(event => event.preventDefault())
				</script>
		    </div>
		</td>
		<td style="vertical-align: top">
			<span id="plots"></span>
		</td>
	</tr>
	<tr>

<!-- Explanations -->
<td colspan=2>
<span class="md" id="rl">

---

## Agent-environment interaction

The __agent__ and __environment__ play a turn-based game and interact in cycles. At time/cycle/turn \\(t\\), the agent supplies the environment with an __action__ \\(a\_t\\). The environment then computes some dynamics on its underlying (and in general, hidden) state \\(s_t\\). It then returns a __percept__ \\(e_t\\) to the agent, and the cycle repeats. In the most general setting, the environment is a __partially observable Markov decision process__.

<p style="text-align: center"><img src="assets/agent-env.png" alt="agent-environment interaction" style="width: 200px"/></p>

The actions live in a (for convenience, stationary) action space \\(\mathcal{A}\\). The percepts live in a (again, stationary) percept space \\(\mathcal{E}\\).

We identify an agent with its __policy__ \\(\pi(a\_t\lvert ae\_{&lt;t})\\)
$$
\pi\ :\ \left(\mathcal{A}\times\mathcal{E}\right)^*\mapsto\varDelta\mathcal{A}
$$

where \\(^*\\) is the [Kleene star][Kleene], and \\(\varDelta \mathcal{X}\\) is the set of probability measures over \\(\mathcal{X}\\).

An environment is a distribution over percepts \\(\nu(e\_t\lvert ae\_{&lt;t}a\_t)\\) with
$$
\nu\ :\  \left(\mathcal{A}\times\mathcal{E}\right)^*\times\mathcal{A}\mapsto\varDelta\mathcal{E}.
$$
The agent and environment interaction induces a distribution over __histories__ \\(\nu^\pi\\):

$$
\nu^\pi\left(ae\_{&lt;t}\right) \stackrel{.}{=}\prod\_{k=1}^{t}\pi\left(a\_k\lvert ae\_{&lt;t}\right)\nu\left(e\_k\lvert ae\_{&lt;k}a\_k\right).
$$

Note that TODO: contextual/conditional etc.

## Reinforcement...

Percepts consist of (__observation__, __reward__) pairs \\(e\_k = \left(o\_k,r\_k\right)\\):
$$
\mathcal{E} = \mathcal{O}\times\mathbb{R}.
$$
Now, introduce the __return__, which is the discounted sum of all future rewards:
$$
R\_t \stackrel{.}{=} \sum\_{k=t}^{\infty}\gamma\_k r\_k,
$$
where \\(\gamma\ :\ \mathbb{N}\mapsto[0,1]\\) is a discount function with convergent sum.  Now, if our agent is rational in the [Von Neumann-Morgenstern][VNM] sense, it should maximize the expected return, which we call the __value__. The value achieved by policy \\(\pi\\) in environment \\(\nu\\) given history \\(ae\_{&lt;t}\\) is defined as
$$
V^{\pi}\_{\nu}\left(ae\_{&lt;t}\right)\stackrel{.}{=}\mathbb{E}^{\pi}\_{\nu}\left[\left.\sum\_{k=t}^{\infty}\gamma\_{k}r\_{k}\right|ae\_{&lt;t}\right]
$$

TODO: write in Bellman (recursive) form.

Let \\(\mu\\) be the true environment. The __optimal value__ is the highest value achieved by any policy in this environment:

$$
V\_{\mu}^{*}\stackrel{.}{=}\max\_{\pi}V\_{\mu}^{\pi}.
$$

AI\\(\mu\\) is the \\(\boldsymbol{\mu}\\)__-optimal policy__ of the __informed agent__:
$$
\pi^{\text{AI}\mu}\stackrel{.}{=}\arg\max\_{\pi}V\_{\mu}^{\pi}.
$$
Unpacking this yields the __expectimax__ expression
$$
a\_{t}^{\text{AI}\mu}=\arg\lim\_{m\to\infty}\max\_{a\_{t}}\sum\_{e\_{t}}\cdots\max\_{a\_{m}}\sum\_{e\_{m}}\sum\_{k=t}^{m}\gamma\_{k}r\_{k}\prod\_{j=t}^{k}\mu\left(e\_{j}\lvert ae\_{&lt;j}a\_{j}\right).
$$
This is just a generalization of [minimax][Minimax] to arbitrary stochastic environments/adversaries. In practice, when planning we approximate this computation with [Monte Carlo tree search][Veness11].

## ...Learning

Clearly we don't know \\(\mu\\) _a priori_ in the general RL setting. Generically, there are two main approaches to learning in the context of RL: __model-based__ and __model-free__. They each make their own sets of assumptions:
* Model-free (e.g. [Q-Learning][QLearning]) generally assume the environment is a finite-state MDP
* Model-based (e.g. __Bayesian learning__) assumes the __realizable case__.

The agents we'll be dealing with are all Bayesian.

## Bayesian reinforcement learning

Assume the realizable case: the true environment \\(\mu\\) is contained in some countable __model class__ \\(\mathcal{M}\\). Now, constructor a __Bayesian mixture__ over \\(\mathcal{M}\\): a [convex linear combination][Convex] of environments:
$$
\xi\left(e\_t\lvert ae\_{&lt;t}a\_t\right)\stackrel{.}{=}\sum\_{\nu\in\mathcal{M}}w\_\nu \nu\left(e\_t\lvert ae\_{&lt;t}a\_t\right).
$$
The weights \\(w\_\nu\equiv\Pr\left(\nu\lvert ae\_{&lt;t}\right)\\) specify the agent's __posterior belief distribution__ over \\(\mathcal{M}\\). By [Cromwell's rule][Cromwell], we require further that the __prior__ weights \\(\Pr(\nu\lvert \epsilon)\\) lie in the interval \\((0,1)\ \forall \nu\in\mathcal{M}\\).

Being Bayesian simply means __updating__ these beliefs according to the product rule of probability:
$$
\Pr(\nu\lvert e\_t) = \frac{\Pr(e\_t\lvert \nu)\Pr(\nu)}{\Pr(e\_t)},
$$
which corresponds to performing the update at each cycle:
$$
w\_\nu\leftarrow\frac{\nu(e\_t)w\_\nu}{\xi(e\_t)}.
$$

## AI\\(\xi\\) and AIXI

AI\\(\xi\\) is the __Bayes-optimal__ agent. That is, it is the policy that is optimal with respect to \\(\xi\\)-expected return:
$$
\pi^{\text{AI}\xi}\stackrel{.}{=}\arg\max\_\pi V^\pi\_\xi.
$$
It is a universal, parameter-free Bayesian agent, whose behavior is completely specified by its model class \\(\mathcal{M}\\) and choice of prior \\(\lbrace w\_\nu\rbrace\_{\nu\in\mathcal{M}}\\). [AIXI][AIXI] uses [Solomonoff's universal prior][Solomonoff]:
$$
w\_\nu = 2^{-K(\nu)},
$$
where \\(K(\nu)\\) is the [Kolmogorov complexity][KC] of environment \\(\nu\\). AIXI is hence the 'active' generalization of Solomonoff induction, which is the optimal (but incomputable) inductive learner.

## Exploration vs exploitation; optimality

The most central issue in reinforcement learning is the __exploration-exploitation dilemma__: TODO

## Q-Learning

TODO

## Knowledge-seeking agents

TODO

## BayesExp

TODO

## Thompson Sampling

## Optimistic AIXI

TODO

## MDL Agent

TODO

## Policy gradients

TODO

## Function approximation/Deep Q-Learning

TODO

## Compress and Control

TODO

<!-- References -->
[KC]: https://en.wikipedia.org/wiki/Kolmogorov_complexity
[QLearning]: https://en.wikipedia.org/wiki/Q-learning
[AIXI]: https://en.wikipedia.org/wiki/AIXI
[Solomonoff]: http://www.scholarpedia.org/article/Algorithmic_probability
[VNM]: https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem
[Minimax]: https://en.wikipedia.org/wiki/Minimax
[Kleene]: https://en.wikipedia.org/wiki/Kleene_star
[Convex]: https://en.wikipedia.org/wiki/Convex_combination
[Cromwell]: https://en.wikipedia.org/wiki/Cromwell%27s_rule

[Leike15]: http://jmlr.org/proceedings/papers/v40/Leike15.pdf
[Veness11]: https://www.jair.org/media/3125/live-3125-5397-jair.pdf

</span>

<span class="md" id="qlearn_exp" style="display:none"></span>

<span class="md" id="bayes_exp" style="display:none"></span>

<span class="md" id="aimu_exp" style="display:none"></span>

<span class="md" id="ksa_exp" style="display:none"></span>

<span class="md" id="thompson_exp" style="display:none"></span>

</td>
</tr>
</table>
<hr />
</div>
</body>
</html>
